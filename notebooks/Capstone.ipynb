{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local notebook - First approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/pmbrull/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/pmbrull/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import spacy\n",
    "from spacy.util import compounding\n",
    "from spacy.util import minibatch\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a3d0a7d5ad</td>\n",
       "      <td>Spent the entire morning in a meeting w/ a ven...</td>\n",
       "      <td>my boss was not happy w/ them. Lots of fun.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>251b6a6766</td>\n",
       "      <td>Oh! Good idea about putting them on ice cream</td>\n",
       "      <td>Good</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c9e8d1ef1c</td>\n",
       "      <td>says good (or should i say bad?) afternoon!  h...</td>\n",
       "      <td>says good (or should i say bad?) afternoon!</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f14f087215</td>\n",
       "      <td>i dont think you can vote anymore! i tried</td>\n",
       "      <td>i dont think you can vote anymore!</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bf7473b12d</td>\n",
       "      <td>haha better drunken tweeting you mean?</td>\n",
       "      <td>better</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  a3d0a7d5ad  Spent the entire morning in a meeting w/ a ven...   \n",
       "1  251b6a6766      Oh! Good idea about putting them on ice cream   \n",
       "2  c9e8d1ef1c  says good (or should i say bad?) afternoon!  h...   \n",
       "3  f14f087215         i dont think you can vote anymore! i tried   \n",
       "4  bf7473b12d             haha better drunken tweeting you mean?   \n",
       "\n",
       "                                 selected_text sentiment  \n",
       "0  my boss was not happy w/ them. Lots of fun.   neutral  \n",
       "1                                         Good  positive  \n",
       "2  says good (or should i say bad?) afternoon!   neutral  \n",
       "3           i dont think you can vote anymore!  negative  \n",
       "4                                       better  positive  "
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df = pd.read_csv('../data/train.csv')\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27486, 4)"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27485, 4)"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove missing values\n",
    "raw_df = raw_df.dropna()\n",
    "raw_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11117  8582  7786]\n",
      "['neutral' 'positive' 'negative']\n"
     ]
    }
   ],
   "source": [
    "print(raw_df['sentiment'].value_counts().values)\n",
    "print(raw_df['sentiment'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f55a7c785c0>"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.countplot(x='sentiment', data=raw_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive:  Oh! Good idea about putting them on ice cream\n",
      "Negative:  i dont think you can vote anymore! i tried\n",
      "Neutral: Spent the entire morning in a meeting w/ a vendor, and my boss was not happy w/ them. Lots of fun.  I had other plans for my morning\n"
     ]
    }
   ],
   "source": [
    "# Let's check one example of each:\n",
    "print(\"Positive:\",raw_df[raw_df['sentiment']=='positive']['text'].values[0])\n",
    "print(\"Negative:\",raw_df[raw_df['sentiment']=='negative']['text'].values[0])\n",
    "print(\"Neutral:\",raw_df[raw_df['sentiment']=='neutral']['text'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive: Good\n",
      "Negative: i dont think you can vote anymore!\n",
      "Neutral: my boss was not happy w/ them. Lots of fun.\n"
     ]
    }
   ],
   "source": [
    "# We could try to guess which words would be selected in the positive and negative case:\n",
    "# For positive we could say \"Good idea\"\n",
    "# For negative: \"i dont think\"\n",
    "print(\"Positive:\",raw_df[raw_df['sentiment']=='positive']['selected_text'].values[0])\n",
    "print(\"Negative:\",raw_df[raw_df['sentiment']=='negative']['selected_text'].values[0])\n",
    "print(\"Neutral:\",raw_df[raw_df['sentiment']=='neutral']['selected_text'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all in all, somehow accurate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_words(text, return_list=True):\n",
    "    lower = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower()) # Convert to lower case\n",
    "    words = lower.split() # Split string into words\n",
    "    words = [w for w in words if w not in stopwords.words(\"english\")] # Remove stopwords\n",
    "    # words = [PorterStemmer().stem(w) for w in words] # stem\n",
    "    words = [lemmatizer.lemmatize(w) for w in words]\n",
    "    words = [w for w in words if len(w) > 1]\n",
    "    words = ' '.join(words)\n",
    "    \n",
    "    return words\n",
    "\n",
    "def get_cache(cache_dir, cache_file):\n",
    "    \n",
    "    # If cache_file is not None, try to read from it first\n",
    "    cache_data = None\n",
    "    if cache_file is not None:\n",
    "        try:\n",
    "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
    "                cache_data = pd.read_pickle(f)\n",
    "            print(\"Read preprocessed data from cache file:\", cache_file)\n",
    "        except:\n",
    "            pass  # unable to read from cache, but that's okay\n",
    "        \n",
    "    return cache_data\n",
    "\n",
    "def clean_data(df):\n",
    "    clean_text = df['text'].apply(text_to_words)\n",
    "    clean_extracted_text = df['selected_text'].apply(text_to_words)\n",
    "\n",
    "    clean_df = pd.DataFrame({\n",
    "        'textID': df['textID'],\n",
    "        'text': clean_text,\n",
    "        'selected_text': clean_extracted_text,\n",
    "        'sentiment': df['sentiment']\n",
    "    })\n",
    "    \n",
    "    return clean_df\n",
    "\n",
    "def preprocess_df(df, cache_dir='../cache', cache_file=\"clean_data.pkl\"):\n",
    "    \n",
    "    os.makedirs(cache_dir, exist_ok=True)  # ensure cache directory exists\n",
    "    \n",
    "    cache_data = get_cache(cache_dir, cache_file)\n",
    "    \n",
    "    if cache_data is None:\n",
    "        cache_data = clean_data(df)\n",
    "        cache_data.to_pickle(os.path.join(cache_dir, cache_file))\n",
    "\n",
    "    return cache_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read preprocessed data from cache file: clean_data.pkl\n"
     ]
    }
   ],
   "source": [
    "# clean_df = preprocess_df(raw_df)\n",
    "clean_df = preprocess_df(raw_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove empty selected text, usually due to just composed by characters\n",
    "clean_df = clean_df.loc[clean_df['selected_text'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>251b6a6766</td>\n",
       "      <td>oh good idea putting ice cream</td>\n",
       "      <td>good</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f14f087215</td>\n",
       "      <td>dont think vote anymore tried</td>\n",
       "      <td>dont think vote anymore</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bf7473b12d</td>\n",
       "      <td>haha better drunken tweeting mean</td>\n",
       "      <td>better</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1915bebcb3</td>\n",
       "      <td>headache wanna see julie</td>\n",
       "      <td>headache</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2ab82634d5</td>\n",
       "      <td>awsome salad recommend getting spicey buffalo ...</td>\n",
       "      <td>awsome salad</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "1  251b6a6766                     oh good idea putting ice cream   \n",
       "3  f14f087215                      dont think vote anymore tried   \n",
       "4  bf7473b12d                  haha better drunken tweeting mean   \n",
       "5  1915bebcb3                           headache wanna see julie   \n",
       "6  2ab82634d5  awsome salad recommend getting spicey buffalo ...   \n",
       "\n",
       "             selected_text sentiment  \n",
       "1                     good  positive  \n",
       "3  dont think vote anymore  negative  \n",
       "4                   better  positive  \n",
       "5                 headache  negative  \n",
       "6             awsome salad  positive  "
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Features from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source of code : https://medium.com/@cristhianboujon/how-to-list-the-most-common-words-from-text-corpus-using-scikit-learn-dad4d0cab41d\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def get_top_n_words(corpus, n=None):\n",
    "    \"\"\"\n",
    "    List the top n words in a vocabulary according to occurrence in a text corpus.\n",
    "    \"\"\"\n",
    "    vec = CountVectorizer(stop_words = 'english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_unigrams = get_top_n_words(clean_df.loc[clean_df['sentiment'] == 'positive']['selected_text'], 10)\n",
    "neg_unigrams = get_top_n_words(clean_df.loc[clean_df['sentiment'] == 'negative']['selected_text'], 10)\n",
    "neutral_unigrams = get_top_n_words(clean_df.loc[clean_df['sentiment'] == 'neutral']['selected_text'], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "x = [x[0] for x in pos_unigrams]\n",
    "y = [x[1] for x in pos_unigrams]ref\n",
    "ax.bar(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "x = [x[0] for x in neg_unigrams]\n",
    "y = [x[1] for x in neg_unigrams]\n",
    "ax.bar(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "x = [x[0] for x in neutral_unigrams]\n",
    "y = [x[1] for x in neutral_unigrams]\n",
    "ax.bar(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "wordcloud = WordCloud(background_color=\"white\", colormap=\"viridis\")\\\n",
    "    .generate(\" \".join(clean_df.loc[clean_df['sentiment'] == 'positive']['text']))\n",
    "fig = plt.figure(figsize=(10,4))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "fig.suptitle('Positive Text', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(background_color=\"white\", colormap=\"inferno\")\\\n",
    "    .generate(\" \".join(clean_df.loc[clean_df['sentiment'] == 'negative']['text']))\n",
    "fig = plt.figure(figsize=(10,4))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "fig.suptitle('Negative Text', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(background_color=\"white\", colormap=\"terrain\")\\\n",
    "    .generate(\" \".join(clean_df.loc[clean_df['sentiment'] == 'neutral']['text']))\n",
    "fig = plt.figure(figsize=(10,4))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "fig.suptitle('Neutral Text', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df['text_word_count'] = clean_df['text'].apply(lambda x: len(str(x).split()))\n",
    "clean_df['selected_text_word_count'] = clean_df['selected_text'].apply(lambda x: len(str(x).split()))\n",
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.loc[clean_df['sentiment'] == 'positive'][['text_word_count', 'selected_text_word_count']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.loc[clean_df['sentiment'] == 'negative'][['text_word_count', 'selected_text_word_count']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.loc[clean_df['sentiment'] == 'neutral'][['text_word_count', 'selected_text_word_count']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition Data - Keep only pos and neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12933, 3) (12933,)\n",
      "(3234, 3) (3234,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "clean_df = clean_df.loc[clean_df['sentiment'] != 'neutral']\n",
    "\n",
    "clean_y = clean_df['selected_text']\n",
    "clean_train_df = clean_df.drop('selected_text', axis=1)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(clean_train_df, clean_y, test_size=0.2)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Model - NER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(output_dir, nlp, new_model_name):\n",
    "    output_dir = f'../working/{output_dir}'\n",
    "    if output_dir is not None:        \n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        nlp.meta[\"name\"] = new_model_name\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_data, output_dir, n_iter=20, model=None):\n",
    "    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n",
    "    \n",
    "    if model is not None:\n",
    "        nlp = spacy.load(output_dir)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank(\"en\")  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "    \n",
    "    # create the built-in pipeline components and add them to the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe(\"ner\")\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "    # otherwise, get it so we can add labels\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "    \n",
    "    # add labels\n",
    "    for _, annotations in train_data:\n",
    "        for ent in annotations.get(\"entities\"):\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        # sizes = compounding(1.0, 4.0, 1.001)\n",
    "        # batch up the examples using spaCy's minibatch\n",
    "        if model is None:\n",
    "            nlp.begin_training()\n",
    "        else:\n",
    "            nlp.resume_training()\n",
    "\n",
    "\n",
    "        for itn in tqdm(range(n_iter)):\n",
    "            random.shuffle(train_data)\n",
    "            batches = minibatch(train_data, size=compounding(4.0, 500.0, 1.001))    \n",
    "            losses = {}\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(texts,  # batch of texts\n",
    "                            annotations,  # batch of annotations\n",
    "                            drop=0.5,   # dropout - make it harder to memorise data\n",
    "                            losses=losses, \n",
    "                            )\n",
    "            print(\"Losses\", losses)\n",
    "    save_model(output_dir, nlp, 'st_ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_out_path(sentiment):\n",
    "    model_out_path = None\n",
    "    if sentiment == 'positive':\n",
    "        model_out_path = 'models/model_pos'\n",
    "    elif sentiment == 'negative':\n",
    "        model_out_path = 'models/model_neg'\n",
    "    return model_out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating data in spacy data input format\n",
    "\n",
    "def get_training_data(df):\n",
    "    train_data = []\n",
    "    for index, row in df.iterrows():\n",
    "        if row.sentiment == sentiment:\n",
    "            selected_text = row.selected_text\n",
    "            text = row.text\n",
    "            start = text.find(selected_text)\n",
    "            end = start + len(selected_text)\n",
    "            train_data.append((text, {\"entities\": [[start, end, 'selected_text']]}))\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pmbrull/venv/vner/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>selected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22822</th>\n",
       "      <td>f94af8cb85</td>\n",
       "      <td>much better tool come across http www tweepula...</td>\n",
       "      <td>positive</td>\n",
       "      <td>much better tool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14445</th>\n",
       "      <td>96de84d102</td>\n",
       "      <td>like saddest person jtv right</td>\n",
       "      <td>negative</td>\n",
       "      <td>saddest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4929</th>\n",
       "      <td>328c362a0b</td>\n",
       "      <td>think come back brisbane australia loved</td>\n",
       "      <td>positive</td>\n",
       "      <td>loved</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8014</th>\n",
       "      <td>e88d4be483</td>\n",
       "      <td>get chat oh well time eat praline</td>\n",
       "      <td>negative</td>\n",
       "      <td>get chat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2194</th>\n",
       "      <td>98cfd2075f</td>\n",
       "      <td>people idea depressing steakhouse able eat</td>\n",
       "      <td>negative</td>\n",
       "      <td>depressing steakhouse</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           textID                                               text  \\\n",
       "22822  f94af8cb85  much better tool come across http www tweepula...   \n",
       "14445  96de84d102                      like saddest person jtv right   \n",
       "4929   328c362a0b           think come back brisbane australia loved   \n",
       "8014   e88d4be483                  get chat oh well time eat praline   \n",
       "2194   98cfd2075f         people idea depressing steakhouse able eat   \n",
       "\n",
       "      sentiment          selected_text  \n",
       "22822  positive       much better tool  \n",
       "14445  negative                saddest  \n",
       "4929   positive                  loved  \n",
       "8014   negative               get chat  \n",
       "2194   negative  depressing steakhouse  "
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train['selected_text'] = y_train\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pmbrull/venv/vner/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>selected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27044</th>\n",
       "      <td>756b4e674b</td>\n",
       "      <td>Stupid competition stuff keeps getting in the ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>Stupid competition stuff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19829</th>\n",
       "      <td>ecfc3c4122</td>\n",
       "      <td>I am beginning to think sun blcok is a haox.</td>\n",
       "      <td>negative</td>\n",
       "      <td>haox.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22526</th>\n",
       "      <td>4bf221814c</td>\n",
       "      <td>Snap. I'm the same with any reality show. Wat...</td>\n",
       "      <td>negative</td>\n",
       "      <td>Very sa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16610</th>\n",
       "      <td>ce1ac5a388</td>\n",
       "      <td>greeting again every momma : Happy Momma's Day!</td>\n",
       "      <td>positive</td>\n",
       "      <td>greeting again every momma : Happy Momma's Day!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14579</th>\n",
       "      <td>a9456542d7</td>\n",
       "      <td>Happy mother's day to all moms out there!  i j...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           textID                                               text  \\\n",
       "27044  756b4e674b  Stupid competition stuff keeps getting in the ...   \n",
       "19829  ecfc3c4122       I am beginning to think sun blcok is a haox.   \n",
       "22526  4bf221814c   Snap. I'm the same with any reality show. Wat...   \n",
       "16610  ce1ac5a388    greeting again every momma : Happy Momma's Day!   \n",
       "14579  a9456542d7  Happy mother's day to all moms out there!  i j...   \n",
       "\n",
       "      sentiment                                    selected_text  \n",
       "27044  negative                         Stupid competition stuff  \n",
       "19829  negative                                            haox.  \n",
       "22526  negative                                          Very sa  \n",
       "16610  positive  greeting again every momma : Happy Momma's Day!  \n",
       "14579  positive                                            Happy  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_train_raw, X_val_raw, y_train_raw, y_val_raw = train_test_split(raw_train_df, raw_y, test_size=0.2)\n",
    "\n",
    "X_train_raw['selected_text'] = y_train_raw\n",
    "X_train_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created blank 'en' model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 1/10 [00:19<02:54, 19.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 15273.011087310908}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 2/10 [00:38<02:34, 19.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 14469.76810618876}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 3/10 [00:58<02:16, 19.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 14073.24419212438}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 4/10 [01:20<02:02, 20.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 13589.363596024465}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 5/10 [01:40<01:40, 20.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 13179.708280419893}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 6/10 [02:00<01:20, 20.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 12924.299197079443}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 7/10 [02:28<01:06, 22.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 12520.775182823032}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 8/10 [02:56<00:48, 24.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 12490.921102098255}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 9/10 [03:24<00:25, 25.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 12520.07851177104}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [03:52<00:00, 23.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 12167.914460011747}\n",
      "Saved model to ../working/models/model_pos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Train positive\n",
    "\n",
    "sentiment = 'positive'\n",
    "train_positive_df = X_train.loc[X_train['sentiment'] == sentiment]\n",
    "\n",
    "train_data = get_training_data(train_positive_df)\n",
    "model_path = get_model_out_path(sentiment)\n",
    "\n",
    "train(train_data, model_path, n_iter=10, model=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created blank 'en' model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 1/10 [00:18<02:49, 18.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 14333.290231212508}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 2/10 [00:37<02:30, 18.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 12907.944787556276}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 3/10 [00:55<02:10, 18.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 12356.595063333429}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 4/10 [01:15<01:53, 18.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 11976.58942808477}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 5/10 [01:33<01:33, 18.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 11488.797077280145}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 6/10 [01:51<01:13, 18.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 11092.223902613972}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 7/10 [02:13<00:58, 19.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 10888.878496573216}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 8/10 [02:39<00:42, 21.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 10522.26824167232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 9/10 [03:05<00:22, 22.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 10462.934786350363}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [03:31<00:00, 21.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 10184.214839122727}\n",
      "Saved model to ../working/models/model_neg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment = 'negative'\n",
    "train_negative_df = X_train.loc[X_train['sentiment'] == sentiment]\n",
    "\n",
    "\n",
    "train_data = get_training_data(train_negative_df)\n",
    "model_path = get_model_out_path(sentiment)\n",
    "\n",
    "train(train_data, model_path, n_iter=10, model=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREDICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pos = spacy.load('../working/models/model_pos')\n",
    "model_neg = spacy.load('../working/models/model_neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_entities(text, model):\n",
    "    doc = model(text)\n",
    "    ent_array = []\n",
    "    for ent in doc.ents:\n",
    "        start = text.find(ent.text)\n",
    "        end = start + len(ent.text)\n",
    "        new_int = [start, end, ent.label_]\n",
    "        if new_int not in ent_array:\n",
    "            ent_array.append([start, end, ent.label_])\n",
    "    selected_text = text[ent_array[0][0]: ent_array[0][1]] if len(ent_array) > 0 else text\n",
    "    return selected_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pmbrull/venv/vner/lib/python3.6/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "selected_texts = []\n",
    "for index, row in X_val.iterrows():\n",
    "    text = row.text\n",
    "    output_str = \"\"\n",
    "    if row.sentiment == 'positive':\n",
    "        selected_texts.append(predict_entities(text, model_pos))\n",
    "    else:\n",
    "        selected_texts.append(predict_entities(text, model_neg))\n",
    "        \n",
    "X_val['selected_text_pred'] = selected_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pmbrull/venv/vner/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "X_val['selected_text'] = y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>selected_text_pred</th>\n",
       "      <th>selected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26479</th>\n",
       "      <td>73b49ab662</td>\n",
       "      <td>watching youtube vid sing paranoid live cute s...</td>\n",
       "      <td>positive</td>\n",
       "      <td>cute</td>\n",
       "      <td>cut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18529</th>\n",
       "      <td>9abcf3cb2c</td>\n",
       "      <td>aww sry yesterday</td>\n",
       "      <td>negative</td>\n",
       "      <td>aww sry yesterday</td>\n",
       "      <td>sry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17776</th>\n",
       "      <td>433cb03f2a</td>\n",
       "      <td>tummy hurt</td>\n",
       "      <td>negative</td>\n",
       "      <td>hurt</td>\n",
       "      <td>tummy hurt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10816</th>\n",
       "      <td>2736c303b4</td>\n",
       "      <td>finally friday still grounded till next thursd...</td>\n",
       "      <td>positive</td>\n",
       "      <td>finally friday still grounded till next thursd...</td>\n",
       "      <td>stereo life lt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22964</th>\n",
       "      <td>2fed83307e</td>\n",
       "      <td>least done</td>\n",
       "      <td>positive</td>\n",
       "      <td>least done</td>\n",
       "      <td>least done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25530</th>\n",
       "      <td>c609f49827</td>\n",
       "      <td>think tricked ankle cardio yesterday getting old</td>\n",
       "      <td>negative</td>\n",
       "      <td>think tricked ankle cardio yesterday getting old</td>\n",
       "      <td>tricked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16863</th>\n",
       "      <td>3696933f3b</td>\n",
       "      <td>awww thanks jon appreciate</td>\n",
       "      <td>positive</td>\n",
       "      <td>awww thanks jon appreciate</td>\n",
       "      <td>awww thanks jon appreciate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17099</th>\n",
       "      <td>ccf98809a7</td>\n",
       "      <td>wow hope get better cancer gtfo</td>\n",
       "      <td>positive</td>\n",
       "      <td>wow hope</td>\n",
       "      <td>wow hope get bette</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15998</th>\n",
       "      <td>0676ff0933</td>\n",
       "      <td>happy mom day mom</td>\n",
       "      <td>positive</td>\n",
       "      <td>happy</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21880</th>\n",
       "      <td>fba650361b</td>\n",
       "      <td>thank</td>\n",
       "      <td>positive</td>\n",
       "      <td>thank</td>\n",
       "      <td>thank</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           textID                                               text  \\\n",
       "26479  73b49ab662  watching youtube vid sing paranoid live cute s...   \n",
       "18529  9abcf3cb2c                                  aww sry yesterday   \n",
       "17776  433cb03f2a                                         tummy hurt   \n",
       "10816  2736c303b4  finally friday still grounded till next thursd...   \n",
       "22964  2fed83307e                                         least done   \n",
       "25530  c609f49827   think tricked ankle cardio yesterday getting old   \n",
       "16863  3696933f3b                         awww thanks jon appreciate   \n",
       "17099  ccf98809a7                    wow hope get better cancer gtfo   \n",
       "15998  0676ff0933                                  happy mom day mom   \n",
       "21880  fba650361b                                              thank   \n",
       "\n",
       "      sentiment                                 selected_text_pred  \\\n",
       "26479  positive                                               cute   \n",
       "18529  negative                                  aww sry yesterday   \n",
       "17776  negative                                               hurt   \n",
       "10816  positive  finally friday still grounded till next thursd...   \n",
       "22964  positive                                         least done   \n",
       "25530  negative   think tricked ankle cardio yesterday getting old   \n",
       "16863  positive                         awww thanks jon appreciate   \n",
       "17099  positive                                           wow hope   \n",
       "15998  positive                                              happy   \n",
       "21880  positive                                              thank   \n",
       "\n",
       "                    selected_text  \n",
       "26479                         cut  \n",
       "18529                         sry  \n",
       "17776                  tummy hurt  \n",
       "10816              stereo life lt  \n",
       "22964                  least done  \n",
       "25530                     tricked  \n",
       "16863  awww thanks jon appreciate  \n",
       "17099          wow hope get bette  \n",
       "15998                       happy  \n",
       "21880                       thank  "
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pmbrull/venv/vner/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>selected_text_pred</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26479</th>\n",
       "      <td>73b49ab662</td>\n",
       "      <td>watching youtube vid sing paranoid live cute s...</td>\n",
       "      <td>positive</td>\n",
       "      <td>cute</td>\n",
       "      <td>cut</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18529</th>\n",
       "      <td>9abcf3cb2c</td>\n",
       "      <td>aww sry yesterday</td>\n",
       "      <td>negative</td>\n",
       "      <td>aww sry yesterday</td>\n",
       "      <td>sry</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17776</th>\n",
       "      <td>433cb03f2a</td>\n",
       "      <td>tummy hurt</td>\n",
       "      <td>negative</td>\n",
       "      <td>hurt</td>\n",
       "      <td>tummy hurt</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10816</th>\n",
       "      <td>2736c303b4</td>\n",
       "      <td>finally friday still grounded till next thursd...</td>\n",
       "      <td>positive</td>\n",
       "      <td>finally friday still grounded till next thursd...</td>\n",
       "      <td>stereo life lt</td>\n",
       "      <td>0.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22964</th>\n",
       "      <td>2fed83307e</td>\n",
       "      <td>least done</td>\n",
       "      <td>positive</td>\n",
       "      <td>least done</td>\n",
       "      <td>least done</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           textID                                               text  \\\n",
       "26479  73b49ab662  watching youtube vid sing paranoid live cute s...   \n",
       "18529  9abcf3cb2c                                  aww sry yesterday   \n",
       "17776  433cb03f2a                                         tummy hurt   \n",
       "10816  2736c303b4  finally friday still grounded till next thursd...   \n",
       "22964  2fed83307e                                         least done   \n",
       "\n",
       "      sentiment                                 selected_text_pred  \\\n",
       "26479  positive                                               cute   \n",
       "18529  negative                                  aww sry yesterday   \n",
       "17776  negative                                               hurt   \n",
       "10816  positive  finally friday still grounded till next thursd...   \n",
       "22964  positive                                         least done   \n",
       "\n",
       "        selected_text     score  \n",
       "26479             cut  0.000000  \n",
       "18529             sry  0.333333  \n",
       "17776      tummy hurt  0.500000  \n",
       "10816  stereo life lt  0.272727  \n",
       "22964      least done  1.000000  "
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val['score'] = X_val[['selected_text','selected_text_pred']].apply(lambda x: jaccard(*x), axis=1)\n",
    "X_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5362184584628107"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final Score\n",
    "X_val['score'].sum() / X_val.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.56335648767548"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Positive Score\n",
    "X_val.loc[X_val['sentiment'] == 'positive']['score'].sum() / X_val.loc[X_val['sentiment'] == 'positive'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5073843024243496"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Negative Score\n",
    "X_val.loc[X_val['sentiment'] == 'negative']['score'].sum() / X_val.loc[X_val['sentiment'] == 'negative'].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aspiring Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pmbrull/venv/vner/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "X_val['selected_text'] = y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "X_train = X_train[['textID', 'text', 'selected_text', 'sentiment']]\n",
    "X_val = X_val[['textID', 'text', 'selected_text', 'sentiment']]\n",
    "\n",
    "train = np.array(X_train)\n",
    "test = np.array(X_val)\n",
    "\n",
    "!mkdir -p data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Prepare training data in QA-compatible format\n",
    "\"\"\"\n",
    "\n",
    "# Adpated from https://www.kaggle.com/cheongwoongkang/roberta-baseline-starter-simple-postprocessing\n",
    "def find_all(input_str, search_str):\n",
    "    l1 = []\n",
    "    length = len(input_str)\n",
    "    index = 0\n",
    "    while index < length:\n",
    "        i = input_str.find(search_str, index)\n",
    "        if i == -1:\n",
    "            return l1\n",
    "        l1.append(i)\n",
    "        index = i + 1\n",
    "    return l1\n",
    "\n",
    "def do_qa_train(train):\n",
    "\n",
    "    output = []\n",
    "    for line in train:\n",
    "        context = line[1]\n",
    "\n",
    "        qas = []\n",
    "        question = line[-1]\n",
    "        qid = line[0]\n",
    "        answers = []\n",
    "        answer = line[2]\n",
    "        if type(answer) != str or type(context) != str or type(question) != str:\n",
    "            print(context, type(context))\n",
    "            print(answer, type(answer))\n",
    "            print(question, type(question))\n",
    "            continue\n",
    "        answer_starts = find_all(context, answer)\n",
    "        for answer_start in answer_starts:\n",
    "            answers.append({'answer_start': answer_start, 'text': answer.lower()})\n",
    "            break\n",
    "        qas.append({'question': question, 'id': qid, 'is_impossible': False, 'answers': answers})\n",
    "\n",
    "        output.append({'context': context.lower(), 'qas': qas})\n",
    "        \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_train = do_qa_train(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove = []\n",
    "for line in qa_train:\n",
    "    if len(line['qas'][0]['answers']) == 0:\n",
    "        to_remove.append(line['qas'][0]['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.loc[X_train['textID'].isin(to_remove) == False]\n",
    "train = np.array(X_train)\n",
    "qa_train = do_qa_train(train)\n",
    "with open('data/train.json', 'w') as outfile:\n",
    "    json.dump(qa_train, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 62.5 ms, sys: 0 ns, total: 62.5 ms\n",
      "Wall time: 79.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\"\"\"\n",
    "Prepare testing data in QA-compatible format\n",
    "\"\"\"\n",
    "\n",
    "def do_qa_test(test):\n",
    "    output = []\n",
    "    for line in test:\n",
    "        context = line[1]\n",
    "        qas = []\n",
    "        question = line[-1]\n",
    "        qid = line[0]\n",
    "        if type(context) != str or type(question) != str:\n",
    "            print(context, type(context))\n",
    "            print(answer, type(answer))\n",
    "            print(question, type(question))\n",
    "            continue\n",
    "        answers = []\n",
    "        answers.append({'answer_start': 1000000, 'text': '__None__'})\n",
    "        qas.append({'question': question, 'id': qid, 'is_impossible': False, 'answers': answers})\n",
    "        output.append({'context': context.lower(), 'qas': qas})\n",
    "    return output\n",
    "\n",
    "qa_test = do_qa_test(test)\n",
    "\n",
    "with open('data/test.json', 'w') as outfile:\n",
    "    json.dump(qa_test, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12790/12790 [00:09<00:00, 1324.54it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d7b2cf81e7146e7829de034eada6dbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=3.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9265933e867b444aa4881db19fde533a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=1599.0, style=ProgressStyle(descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.610161"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pmbrull/venv/vner/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:224: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 1.295430\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5072c64e713436b99cecfed7316f848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=1599.0, style=ProgressStyle(descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 1.162603"
     ]
    }
   ],
   "source": [
    "from simpletransformers.question_answering import QuestionAnsweringModel\n",
    "\n",
    "MODEL_PATH = './pretrained/distilbert-base-uncased-distilled-squad/'\n",
    "\n",
    "# Create the QuestionAnsweringModel\n",
    "model = QuestionAnsweringModel('distilbert', \n",
    "                               MODEL_PATH, \n",
    "                               args={'reprocess_input_data': True,\n",
    "                                     'overwrite_output_dir': True,\n",
    "                                     'learning_rate': 5e-5,\n",
    "                                     'num_train_epochs': 3,\n",
    "                                     'max_seq_length': 192,\n",
    "                                     'doc_stride': 64,\n",
    "                                     'fp16': False,\n",
    "                                    },\n",
    "                              use_cuda=False)\n",
    "\n",
    "model.train_model('data/train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
