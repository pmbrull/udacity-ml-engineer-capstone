\documentclass[11pt]{article}

\usepackage{amsmath}
\usepackage{hyperref}

\usepackage{titling}
\newcommand{\subtitle}[1]{%
  \posttitle{%
    \par\end{center}
    \begin{center}\large#1\end{center}
    \vskip0.5em}%
}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SET THE TITLE
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Tweet Sentiment Extraction}
\subtitle{Udacity ML Engineer Capstone Project}
\author{Brull Borr√†s, Pere Miquel}
   
\date{\today}

\begin{document}
\maketitle

% --------------------
\section{Domain Background}
% --------------------

Data is all around us, and with the introduction of Social Networks, the most straight forward to share this data is via text. This means that we need to find ways to interpret unstructured and free information such as \textbf{language}. When being introduced into \textbf{Natural Language Processing}, Sentiment Analysis is the go-to example one would try to tackle in the form of \textit{tweets}. The statement behind it revolves around inferring if a rather short message could be classified as positive, negative or neutral, based on the subjective information of the language: is the author using "\textit{good} or "\textit{bad}" words? Can we try to predict the emotional state of the author based on such small piece of information?

What we are going to do, however, is look at this same problem from a different perspective. Given the sentiment of the author, can we actually know which words make a message being positive or negative?

% --------------------
\section{Problem Statement}
% --------------------

Our main objective is exploring different NLP techniques that could aid us into finding which parts of the message transmit the \textit{sentiment} information and classify those into positive, negative or neutral. This study is based on the \href{https://www.kaggle.com/c/tweet-sentiment-extraction}{Kaggle's Tweet Sentiment Extraction} competition.

We are going to use the \href{https://www.kaggle.com/c/tweet-sentiment-extraction/data}{train data} from the Kaggle platform. As the submission to the competition is out of the scope, we are going to use this same dataset as training and validation with a 80 / 20 split.

The train data is composed by 27.486 rows with the following information:
\begin{itemize}
    \item textID - Unique identifier of the row
    \item text - Complete tweet
    \item selected text - The text that supports the tweet's sentiment. It is a subset of the \textit{text} feature containing one extract from a sentence. I.e., it does not contain independent words from different parts of the tweet.
    \item sentiment - The generated sentiment of the text, which can be \textit{positive}, \textit{negative} or \textit{neutral}.
\end{itemize}

An \textbf{example} of some positive data is the following:

\begin{verbatim}
  Oh! Good idea about putting them on ice cream.
\end{verbatim}

Which has the following selected text:

\begin{verbatim}
  Good
\end{verbatim}

% --------------------
\section{Evaluation Metrics}
% --------------------

A key point that we need to define is how are we going to score our predicted outputs. In order to do that we will use the \href{https://en.wikipedia.org/wiki/Jaccard_index}{Jaccard index}:

\[
J(A,B) = {{|A \cap B|}\over{|A \cup B|}} = {{|A \cap B|}\over{|A| + |B| - |A \cap B|}}
\]

In our case, the sets $A$ and $B$ are the two sentences which we are comparing after \textit{lemmatization}, so that words such as \textit{friend} and \textit{friendly} are treated as equal. Therefore, following the discussion on the example above, if we would have received the output

\begin{verbatim}
  Good idea
\end{verbatim}

instead of the real selected text

\begin{verbatim}
  Good
\end{verbatim}

The result of the Jaccard index would be:

\[
J(A,B) = {{\{\text{Good}\}}\over{\{\text{Good}, \text{idea}\}}} = 0.5
\]

This way we are penalizing scenarios where we are not finding out enough information or where we are considering too much words that could have been avoided.

% --------------------
\section{Exploratory Data Analysis}
% --------------------

In this section we are going to review the train data and try to answer the following questions:

\begin{itemize}
    \item Is the data well balanced?
    \item What are the most common words for each sentiment?
    \item Are there any differences in the length of the selected texts for each sentiment?
\end{itemize}

TODO: visulization

% --------------------
\section{Solution Statement}
% --------------------

We will explore different NLP techniques based on extracting information, explain and apply them.

For the data preprocessing, we will apply the usual tools such as 

\begin{itemize}
    \item \textbf{Lemmatization} - In order to treat as one the different derived forms from a single term. E.g., "friend", "friends" and "friendly" will all be treated as "friend". We are choosing lemmatization over stemming in order to end up with words that NLP techniques can extract meaning from. Note that if we were to use stemming techniques, words would be truncated to their \textit{root} or \textit{stem}, which is the part of the word that contains the global meaning, disregarding derived forms as gender and number. However, it is possible that we end up with words from which algorithms cannot interpret. Taking the word "studies" as an example, its lemma is "study", while the stem is just "studi".
    \item \textbf{Stopwords removal} - Which are terms that lack of their own meaning and are rather user to build up a sentence in a way that it's easier to share its message.
    \item \textbf{Normalize} - Convert all words to lower case.
\end{itemize}

After cleaning the data, results will be cached in order to speed up future iterations. Then, we will split the train CSV data into our real train data and validation data as 80 / 20.

After fitting and comparing results over the validation set applying the Jaccard index, we will compare the different models.

% --------------------
\section{Benchmark Model}
% --------------------

Our starting point will be \textbf{Named Entity Recognition} as the simplest approach into extracting information from free text.
With the same processed data, we will apply different models and compare their results against NER to see if we can obtain better results.

% --------------------
\section{Aspiring Model}
% --------------------

% --------------------
\section{Conclusion}
% --------------------

% --------------------
\section{References}
% --------------------

\begin{itemize}
    \item Examples of model implementations have been extracted from the Kaggle's notebooks of the competition. We would like to highlight the following: \href{https://www.kaggle.com/rohitsingh9990/ner-inference-using-spacy-lb-0-628}{NER inference using Spacy}.
    \item \href{https://blog.bitext.com/what-is-the-difference-between-stemming-and-lemmatization/}{What is the difference between stemming and lemmatization}.
\end{itemize}

\end{document}
